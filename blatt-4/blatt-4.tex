\documentclass[a4paper,12pt]{article} 

\usepackage[top = 2.5cm, bottom = 2.5cm, left = 2.5cm, right = 2.5cm]{geometry} 

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{booktabs} 
\usepackage{tikz}
\usepackage{float,graphicx} 
\usepackage{amssymb} 
\usepackage{pdfpages}
\usepackage{adjustbox}
\usepackage{setspace}
\setlength{\parindent}{0in}

\usepackage{float}

\usepackage{fancyhdr}
\usepackage{enumerate}


\pagestyle{fancy} 
\fancyhf{}
\lhead{\footnotesize PR \& ML: Blatt 4}
\rhead{\footnotesize F. Freter, E. Kirchberger, S. Symhoven \& J. Wustl} %<---- Fill in your lastnames.

\cfoot{\footnotesize \thepage} 

\begin{document}

\thispagestyle{empty} 

\begin{tabular}{p{15.5cm}} 
{\large \bf Pattern Recognition und Machine Learning} \\
Hochschule München \\ Sommersemester 2023  \\ Prof. Dr.-Ing. Claudius Schnörr \\
\hline 
\\
\end{tabular} 

\vspace*{0.3cm} 

\begin{center} 
	{\Large \bf Blatt 4} 
	\vspace{2mm}
	

	{\bf F. Freter, E. Kirchberger, S. Symhoven \& J. Wustl} 
		
\end{center}  

\vspace{0.4cm}

\includepdf[pages=1,pagecommand=\section*{Aufgabe 1/2: Lineare Regression}]{aufgabe-1-2.pdf}
\includepdf[pages={2-}]{aufgabe-1-2.pdf}

\section*{Aufgabe 3: Support Vektor Maschinen}

\subsection*{SVM-Klassifikation:}

\begin{enumerate}

	\item Der Vektor $y$ repräsentiert die Klassenzuordnungen der Trainingsdaten in einem SVM-Klassifikator. 
		In diesem Fall werden Muster der Klasse 1 mit dem Wert 1 gekennzeichnet und Muster der Klasse 2 mit dem Wert -1 gekennzeichnet. 
		Die Reihenfolge der Elemente im Vektor $y$ entspricht der Reihenfolge der entsprechenden Muster in den Datenvektoren $x_1$ bis $x_7$.
		
		\[ y= \begin{pmatrix} -1 & 1 & 1 & -1 & 1 & -1 & -1 \end{pmatrix}^T \]
	
	
	\item
		\begin{figure}[H]
			\centering
			\includegraphics[width = .7\linewidth]{aufgabe-3-2.png}
	  	\end{figure}

		Ja, es handelt sich dabei um eine linear trennbare Problemstellung. Diese kann über zwei Punkte 
		$a=\begin{pmatrix} 0 \\ 1 \end{pmatrix}$ und $b=\begin{pmatrix} 3.5 \\ 3 \end{pmatrix}$ definiert werden 
		und maximiert den Rand zwsichen den beiden Klassen.  
	

	\item 
	
	Um den Gewichtungsvektor w zu berechnen, verwenden wir normalerweise den Ansatz der Support Vector Machine und die Methode der Lagrange-Multiplikatoren. 
	Der Gewichtungsvektor w wird während des Trainingsprozesses der SVM bestimmt und hängt von den Datenpunkten und den Lagrange-Multiplikatoren ab.

	In unserer zeichnerischen Lösung basierend auf den Punkten $a$ und $b$, haben wir bereits eine Trennfunktion definiert. 
	Die Koeffizienten der Geradengleichung, die die beiden Punkte $a$ und $b$ verbindet, bestimmen die Parameter der linearen Trennfunktion:

	Der Schwellenwert $b$ ist der y-Achsenabschnitt und somit die zweite Komponente von dem Punkt $a$.
	
	\[b = 1\]
	
	Der Steigungsfaktor $w$ der linearen Trennfunktion wird berechnet als:

	\[w = (b_1 - a_1) / (b_0 - a_0) = (3 - 1) / (3.5 - 0) = 2/3.5\]



	\item Daraus ergeben sich die Support-Vektorn $x_2$, $x_5$, $x_6$ und $x_7$.
	
	\item
	
	\item
	
	\item


\end{enumerate}


\subsection*{SVM-Regression:}

\end{document}
